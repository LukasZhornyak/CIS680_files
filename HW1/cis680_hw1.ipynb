{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "%matplotlib inline\n",
    "rng_seed = 1144\n",
    "\n",
    "# Download MNIST\n",
    "torchvision.datasets.MNIST('.', download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CIS680: Assignment 1: Deep Learning Basics\n",
    "### Due:\n",
    "* Part (a) Sept. 12 at 11:59 p.m.\n",
    "* Part (b) Sept. 12 at 11:59 p.m.\n",
    "\n",
    "### Instructions:\n",
    "* Part (a) consists of parts 1, 2 and 3, and is due on September 12 at 11:59 p.m. EDT.\n",
    "* Part (b) consists of part 4 and is due on September 15 at 11:59 p.m. EDT\n",
    "* This is a group assignment with one submission per group. It is expected that each member of the group will contribute\n",
    "to solving each question. Be sure to specify your teammates when you submit to Gradescope! Collaborating with other\n",
    "groups is not permitted.\n",
    "* There is no single answer to most problems in deep learning, therefore the questions will\n",
    "often be underspecified. You need to fill in the blanks and submit a solution that solves the\n",
    "(practical) problem. Document the choices (hyperparameters, features, neural network\n",
    "architectures, etc.) you made where specified.\n",
    "* All the code should be written in Python. You should use PyTorch only to complete this\n",
    "homework.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Plot Loss and Gradient\n",
    "In this part, you will write code to plot the output and gradient for a single neuron with\n",
    "Sigmoid activation and two different loss functions. As shown in Figure 1, You should\n",
    "implement a single neuron with input 1, and calculate different losses and corresponding\n",
    "error.\n",
    "\n",
    "<div><img src=\"https://github.com/LukasZhornyak/CIS680_files/raw/e676f49897a77eb8d1774057e8ea5a216f0dc273/HW1/images/fig1.png\" width=1200/></div>\n",
    "\n",
    "<center>Figure 1: Network diagram for part 1.</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "All the figures plotted in this part should have the same range of x-axis and y-axis. The\n",
    "range should be centered at 0 but the extend should be picked so as to see the difference\n",
    "clearly.\n",
    "\n",
    "A set of example plots are provided in Figure 2. Here we use ReLU (instead of Sigmoid)\n",
    "activation and L2 loss as an example.\n",
    "\n",
    "<div><img src=\"https://github.com/LukasZhornyak/CIS680_files/raw/e676f49897a77eb8d1774057e8ea5a216f0dc273/HW1/images/fig2.png\" width=800/></div>\n",
    "<center>Figure 2: Example plots with ReLU activation and L2 loss. Left: Output of ReLU function.\n",
    "Middle: Loss plot with L2 loss. Right: Gradient plot.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "1. (3%) Plot a 3D figure showing the relations of output of Sigmoid function and weight/bias. To be specific, x-axis is weight, y-axis is bias, and z-axis is the out-put.\n",
    "\n",
    " Hint: Use the Python package matplotlib and the function plot surface from mpl toolkits.mplot3d\n",
    "to draw 3D figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "2. (3%) Experiment with L2 loss. The L2 loss is defined as $\\mathcal{L}_{L2} = (\\hat{y} âˆ’ y)^2$, where $y$ is\n",
    "the ground truth and $\\hat{y}$ is the prediction. Let $y = 0.5$ and plot a 3D figure showing\n",
    "2 the relations of L2 loss and weight/bias. To be specific, the x-axis is weight, y-axis is\n",
    "bias, and z-axis is the L2 loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "3. (4%) Experiment with back-propagation with L2 loss. Compute $\\frac{\\partial \\mathcal{L}_{L2}}{\\partial \\text{weight}}$ and plot a 3D figure showing the relations of gradient and weight/bias. To be specific, the x-axis is weight, y-axis is bias, and z-axis is the gradient w.r.t. weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "4. (3%) Experiment with cross-entropy loss. The cross-entropy loss is defined as $\\mathcal{L}_{CE} = -(y \\log{\\hat{y}} + (1 - y)\\log{(1 - \\hat{y})})$, where $y$ is the ground truth probability and $\\hat{y}$ is the\n",
    "predicted probability. Let $y = 0.5$ and plot a 3D figure showing the relations of\n",
    "cross-entropy loss and weight/bias. To be specific, the x-axis is weight, y-axis is bias,\n",
    "and z-axis is the cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "5. (4%) Experiment with back-propagation with cross-entropy loss. Compute $\\frac{\\partial \\mathcal{L}_{CE}}{\\partial \\text{weight}}$ and plot a 3D figure showing the relations of gradient and weight/bias. To be specific, the x-axis is weight, y-axis is bias, and z-axis is the gradient w.r.t. weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "6. (3%) Explain what you observed from the above 5 plots. The explanation should include: \n",
    " 1. What's the difference between cross-entropy loss and L2 loss?\n",
    " 2. What's the difference between the gradients from cross-entropy loss and L2 loss?\n",
    " 3. Predict how these differences will influence the efficiency of learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Solving XOR with a 2-layer Perceptron (20%)\n",
    "In this question you are asked to build and visualize a 2-layer perceptron that computes\n",
    "the XOR function. The network architecture is shown in Figure 3. The MLP has 1 hidden\n",
    "layer with 2 neurons. The activation function used for the hidden layer is the hyperbolic\n",
    "tangent function. Since we aim to model a boolean function the output of the last layer is\n",
    "passed through a sigmoid activation function to constrain it between 0 and 1.\n",
    "\n",
    "<div><img src=\"https://github.com/LukasZhornyak/CIS680_files/raw/e676f49897a77eb8d1774057e8ea5a216f0dc273/HW1/images/fig3.png\" width=800/></div>\n",
    "<center>Figure 3: Graphical representation of the 2-layer Perceptron</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "1. (5%) Formulate the XOR approximation as an optimization problem using the cross\n",
    "entropy loss. _Hint: Your dataset consists of just 4 points, $x_1 = (0,0)$, $x_2 = (0,1)$,\n",
    "$x_3 = (1,0)$ and $x_4 = (1,1)$ with ground truth labels 0, 1, 1 and 0 respectively._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "2. (10%) Use gradient descent to learn the network weights that optimize the loss. Intuitively, the 2 layer perceptron first performs a nonlinear mapping from $(x_1,x_2) \\rightarrow (h_1,h_2)$ and then learns a linear classifier in the $(h_1,h_2)$ plane. For different steps\n",
    "during training visualize the image of each input point $x_i$ in the $(h_1,h_2)$ plane as well\n",
    "as the decision boundary (separating line) of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make your dataset here\n",
    "data = ...\n",
    "labels = ...\n",
    "\n",
    "# Make your network here\n",
    "network = ...\n",
    "\n",
    "# Train and plot here\n",
    "for i in range(1000):\n",
    "    ...\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "3. (5%) What will happen if we don't use an activation function in the hidden layer? Is\n",
    "the network be able to learn the XOR function? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Train a Convolutional Neural Network (30%)\n",
    "In this part you will be asked to train a convolutional neural network on the MNIST\n",
    "dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "1. (10%) Build a Convolutional Neural Network with architecture as shown below:\n",
    "| Layers | Hyper-parameters |\n",
    "| :--- | :--- |\n",
    "| Covolution 1 | Kernel size $= (5, 5, 32)$, SAME padding. Followed by BatchNorm and ReLU. |\n",
    "| Pooling 1 | Average operation. Kernel size $= (2, 2)$. Stride $= 2$. Padding $= 0$. |\n",
    "| Covolution 2 | Kernel size $= (5, 5, 32)$, SAME padding. Followed by BatchNorm and ReLU. |\n",
    "| Pooling 2 | Average operation. Kernel size $= (2, 2)$. Stride $= 2$. Padding $= 0$. |\n",
    "| Covolution 3 | Kernel size $= (5, 5, 64)$, SAME padding. Followed by BatchNorm and ReLU. |\n",
    "| Pooling 3 | Average operation. Kernel size $= (2, 2)$. Stride $= 2$. Padding $= 0$. |\n",
    "| Fully Connected 1 | Output channels $= 64$. Followed by BatchNorm and ReLU. |\n",
    "| Fully Connected 2 | Output channels $= 10$. Followed by Softmax. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "# Create your network here\n",
    "class DigitClassification(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        ...\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        ...\n",
    "        return x\n",
    "\n",
    "# Instantiate your network here\n",
    "model = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "2. (20%) Train the CNN on the MNIST dataset using the Cross Entropy loss. Report training and testing curves. Your model should reach $99%$ accuracy on the\n",
    "test dataset. (Hint: Normalize the images in the $(-1,1)$ range and use the Adam\n",
    "optimizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Where your trained model will be saved (and where the autograder will load it)\n",
    "model_path = 'model.pth'\n",
    "\n",
    "# Adds your model to the output zip\n",
    "grader.add_plugin_files(\"cis680_plugins.IncludeModelPlugin\", model_path)\n",
    "\n",
    "# Do not edit the line below, everything after it will be skipped by the autograder\n",
    "# Do not attempt to train your model on the autograder, you will be timed out\n",
    "## TRAINING_CODE\n",
    "\n",
    "# Train your network here\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "  print(\"Epoch %d/%d\" % (epoch+1, num_epochs))\n",
    "\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
